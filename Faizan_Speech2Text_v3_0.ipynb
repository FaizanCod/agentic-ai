{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaizanCod/agentic-ai/blob/master/Faizan_Speech2Text_v3_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q yt-dlp\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q youtube-transcript-api # NEW: Library for fast caption extraction\n",
        "!pip install -q torch torchaudio ffmpeg-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQHyaIcsDQBr",
        "outputId": "d6ad98ef-f498-4234-d449-220e4fbc1f75"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m485.1/485.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import yt_dlp\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import sys\n",
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
        "from google.colab import drive"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "id": "BJ8UwSHUAvMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIGURATION ---\n",
        "YOUTUBE_URL = \"https://www.youtube.com/watch?v=h3M00JI8Iwo\"  # <-- REPLACE with your video URL\n",
        "OUTPUT_DIR = \"/content/transcription_output/\"\n",
        "AUDIO_FILE = os.path.join(OUTPUT_DIR, \"audio.mp3\")\n",
        "JSON_OUTPUT_FILE_NAME = \"video_analysis.json\"\n",
        "JSON_OUTPUT_FILE_PATH = os.path.join(OUTPUT_DIR, JSON_OUTPUT_FILE_NAME)"
      ],
      "metadata": {
        "id": "Q6hJdmnyC4qi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- GEMINI AGENT SETUP ---\n",
        "# Note: In a Colab environment, you typically need to set the API_KEY environment variable\n",
        "# manually or rely on the host environment's configuration.\n",
        "# For simplicity, we use the standard setup assuming requests will handle the key if available.\n",
        "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent\"\n",
        "API_KEY = \"AIzaSyCwWhwTK_a84BvvpvB602m__IJxx76yB5M\""
      ],
      "metadata": {
        "id": "GlgUQjuEC7Mi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# AGENT TOOL 1: UTILITIES\n",
        "# =================================================================\n",
        "\n",
        "def install_dependencies():\n",
        "    \"\"\"Installs required Python packages quietly.\"\"\"\n",
        "    print(\"--- Installing Dependencies ---\")\n",
        "    try:\n",
        "        os.system('pip install -q yt-dlp')\n",
        "        os.system('pip install -q git+https://github.com/openai/whisper.git')\n",
        "        os.system('pip install -q youtube-transcript-api')\n",
        "        os.system('pip install -q torch torchaudio ffmpeg-python')\n",
        "    except Exception as e:\n",
        "        print(f\"Error during installation: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "def get_video_id(url):\n",
        "    \"\"\"Extracts the YouTube video ID from a given URL.\"\"\"\n",
        "    match = re.search(r\"(?<=v=)[a-zA-Z0-9_-]{11}|(?<=youtu\\.be/)[a-zA-Z0-9_-]{11}|(?<=/embed/)[a-zA-Z0-9_-]{11}\", url)\n",
        "    return match.group(0) if match else None\n",
        "\n",
        "def format_time(seconds):\n",
        "    \"\"\"Converts seconds to HH:MM:SS format.\"\"\"\n",
        "    seconds = int(seconds)\n",
        "    hours, remainder = divmod(seconds, 3600)\n",
        "    minutes, seconds = divmod(remainder, 60)\n",
        "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
        "\n",
        "def group_segments_by_context(segments, max_gap_sec=2.0):\n",
        "    \"\"\"\n",
        "    Merges segments into contextually complete blocks based on time gaps.\n",
        "    Returns segments with unified 'start' and 'end' keys.\n",
        "    \"\"\"\n",
        "    if not segments:\n",
        "        return []\n",
        "\n",
        "    grouped_output = []\n",
        "    # Initialize the first group\n",
        "    current_group = {\n",
        "        \"start\": segments[0][\"start\"],\n",
        "        \"text\": segments[0][\"text\"],\n",
        "        \"end\": segments[0][\"end\"]\n",
        "    }\n",
        "\n",
        "    for i in range(1, len(segments)):\n",
        "        prev_end = current_group[\"end\"]\n",
        "        current_start = segments[i][\"start\"]\n",
        "\n",
        "        # If the gap is small, merge the text and extend the end time of the current group\n",
        "        if (current_start - prev_end) < max_gap_sec:\n",
        "            current_group[\"text\"] += \" \" + segments[i][\"text\"]\n",
        "            current_group[\"end\"] = segments[i][\"end\"]\n",
        "        else:\n",
        "            # If the gap is large, finalize the current group and start a new one\n",
        "            grouped_output.append(current_group)\n",
        "            current_group = {\n",
        "                \"start\": current_start,\n",
        "                \"text\": segments[i][\"text\"],\n",
        "                \"end\": segments[i][\"end\"]\n",
        "            }\n",
        "\n",
        "    # Append the last remaining group\n",
        "    grouped_output.append(current_group)\n",
        "    return grouped_output\n",
        "\n",
        "def split_segments_by_time(segments, max_segment_duration=60.0):\n",
        "    \"\"\"\n",
        "    Splits any segment longer than max_segment_duration into smaller, uniform chunks.\n",
        "    This is necessary when the source API returns the entire transcript as one segment.\n",
        "    \"\"\"\n",
        "    if not segments:\n",
        "        return []\n",
        "\n",
        "    chunked_output = []\n",
        "\n",
        "    for seg in segments:\n",
        "        segment_duration = seg['end'] - seg['start']\n",
        "\n",
        "        # Check if the segment is already short enough or has no duration (skip)\n",
        "        if segment_duration <= max_segment_duration or segment_duration <= 0:\n",
        "            chunked_output.append(seg)\n",
        "            continue\n",
        "\n",
        "        # If segment is too long, calculate how many chunks are needed\n",
        "        num_chunks = int(segment_duration // max_segment_duration)\n",
        "        remainder_duration = segment_duration % max_segment_duration\n",
        "\n",
        "        current_time = seg['start']\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            chunk_start = current_time\n",
        "            chunk_end = chunk_start + max_segment_duration\n",
        "\n",
        "            # Estimate where the text should be split (simple duration division)\n",
        "            text_length = len(seg['text'])\n",
        "            start_ratio = (chunk_start - seg['start']) / segment_duration\n",
        "            end_ratio = (chunk_end - seg['start']) / segment_duration\n",
        "\n",
        "            start_index = int(text_length * start_ratio)\n",
        "            end_index = int(text_length * end_ratio)\n",
        "\n",
        "            chunk_text = seg['text'][start_index:end_index].strip()\n",
        "\n",
        "            chunked_output.append({\n",
        "                \"start\": chunk_start,\n",
        "                \"end\": chunk_end,\n",
        "                \"text\": chunk_text if chunk_text else \"...\"\n",
        "            })\n",
        "            current_time = chunk_end\n",
        "\n",
        "        # Handle the remainder chunk\n",
        "        if remainder_duration > 0:\n",
        "            chunked_output.append({\n",
        "                \"start\": current_time,\n",
        "                \"end\": seg['end'],\n",
        "                \"text\": seg['text'][int(text_length * (current_time - seg['start']) / segment_duration):].strip()\n",
        "            })\n",
        "\n",
        "    return chunked_output\n",
        "\n",
        "def upload_to_drive(local_path, file_name):\n",
        "    \"\"\"Mounts Google Drive and copies the file to a standard location.\"\"\"\n",
        "    print(\"\\n--- Starting Google Drive Upload ---\")\n",
        "\n",
        "    # 1. Mount Google Drive\n",
        "    drive_mount_path = \"/content/gdrive\"\n",
        "    try:\n",
        "        drive.mount(drive_mount_path, force_remount=True)\n",
        "    except Exception as e:\n",
        "        print(f\"üî¥ Error mounting Google Drive: {e}\")\n",
        "        return False\n",
        "\n",
        "    # 2. Define the destination path in your Drive\n",
        "    drive_destination_folder = os.path.join(drive_mount_path, 'MyDrive', 'AI_Transcripts')\n",
        "\n",
        "    if not os.path.exists(drive_destination_folder):\n",
        "        os.makedirs(drive_destination_folder)\n",
        "        print(f\"Created destination folder: {drive_destination_folder}\")\n",
        "\n",
        "    # 3. Copy the file using a shell command (cp)\n",
        "    drive_destination_path = os.path.join(drive_destination_folder, file_name)\n",
        "\n",
        "    # Use !cp command in Colab environment\n",
        "    try:\n",
        "        os.system(f'cp \"{local_path}\" \"{drive_destination_path}\"')\n",
        "\n",
        "        print(\"\\n-------------------------------------------\")\n",
        "        print(\"‚úÖ Upload Successful!\")\n",
        "        print(f\"File saved to Google Drive at: MyDrive/AI_Transcripts/{file_name}\")\n",
        "        print(\"-------------------------------------------\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"üî¥ Error copying file to Drive: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "mM2tFJJtD9WU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# AGENT TOOL 2: TRANSCRIPTION & FALLBACK LOGIC\n",
        "# =================================================================\n",
        "\n",
        "def run_youtube_caption_api(video_id):\n",
        "    \"\"\"Attempts to fetch English captions directly from YouTube.\"\"\"\n",
        "    try:\n",
        "        # 1. Get the list of available transcripts\n",
        "        ytt_api = YouTubeTranscriptApi()\n",
        "\n",
        "        # 2. Find the English transcript (prioritizing manually created over auto-generated)\n",
        "        fetched_transcript = ytt_api.fetch(video_id, languages=['en']).to_raw_data()\n",
        "\n",
        "        raw_api_segments = []\n",
        "        for item in fetched_transcript:\n",
        "            # FIX: Access attributes directly (item.text) instead of dict syntax (item['text'])\n",
        "            # The API returns objects with .text, .start, and .duration attributes\n",
        "            segment = {\n",
        "                \"text\": item['text'],\n",
        "                \"start\": item['start'],\n",
        "                \"end\": item['start'] + item['duration']\n",
        "            }\n",
        "            raw_api_segments.append(segment)\n",
        "\n",
        "        print(\"‚úÖ Success: Retrieved segments using YouTube Captions API.\")\n",
        "        return raw_api_segments\n",
        "\n",
        "    except (TranscriptsDisabled, NoTranscriptFound) as e:\n",
        "        print(f\"‚ùå Fallback needed: Native YouTube captions not found or disabled.\")\n",
        "        return None\n",
        "    except TypeError:\n",
        "        # Fallback for version mismatch where item might be an object, not a dict\n",
        "        # Re-attempting loop assuming object structure if dict access failed\n",
        "        try:\n",
        "            raw_api_segments = []\n",
        "            for item in fetched_transcript:\n",
        "                segment = {\n",
        "                  \"text\": item['text'],\n",
        "                  \"start\": item['start'],\n",
        "                  \"end\": item['start'] + item['duration']\n",
        "                }\n",
        "                raw_api_segments.append(segment)\n",
        "            print(\"‚úÖ Success: Retrieved segments using YouTube Captions API (Object Mode).\")\n",
        "            return raw_api_segments\n",
        "        except Exception as e:\n",
        "             print(f\"‚ùå Fallback needed: Error parsing transcript objects: {e}\")\n",
        "             return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Fallback needed: An unexpected API error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "def run_whisper_transcription(audio_file):\n",
        "    \"\"\"Runs local Whisper model transcription on the downloaded audio.\"\"\"\n",
        "    if not os.path.exists(audio_file):\n",
        "        raise FileNotFoundError(f\"Audio file not found at: {audio_file}\")\n",
        "\n",
        "    print(\"Starting Whisper transcription... (This may take several minutes)\")\n",
        "    try:\n",
        "        # Load the base model (fastest). Change to 'medium' or 'large' for higher accuracy.\n",
        "        model = whisper.load_model(\"base\")\n",
        "        print(\"Whisper model loaded successfully.\")\n",
        "\n",
        "        result = model.transcribe(audio_file, verbose=True, word_timestamps=True)\n",
        "\n",
        "        raw_whisper_segments = []\n",
        "        for segment in result['segments']:\n",
        "            raw_whisper_segments.append({\n",
        "                \"start\": segment['start'],\n",
        "                \"end\": segment['end'],\n",
        "                \"text\": segment['text'].strip()\n",
        "            })\n",
        "\n",
        "        print(f\"‚úÖ Success: Transcribed {len(raw_whisper_segments)} raw segments using Whisper.\")\n",
        "        return raw_whisper_segments\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"üî¥ Fatal Error during Whisper Transcription: {e}\")\n",
        "        return None\n",
        "\n",
        "def download_audio_with_yt_dlp(url, final_output_path):\n",
        "    \"\"\"Downloads audio, handles file naming, and ensures the file exists at final_output_path.\"\"\"\n",
        "    print(f\"Downloading audio from {url}...\")\n",
        "\n",
        "    # We use a temporary base name to avoid confusion with existing files\n",
        "    temp_base_name = \"temp_download\"\n",
        "    output_dir = os.path.dirname(final_output_path)\n",
        "    temp_output_template = os.path.join(output_dir, f\"{temp_base_name}.%(ext)s\")\n",
        "\n",
        "    # Define yt-dlp options for audio extraction and conversion\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'outtmpl': temp_output_template,\n",
        "        'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3', 'preferredquality': '192'}],\n",
        "        'logger': None, 'quiet': True, 'no_warnings': True,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            ydl.download([url])\n",
        "\n",
        "        # After download, yt-dlp (with ffmpeg) should have created 'temp_download.mp3'\n",
        "        expected_temp_file = os.path.join(output_dir, f\"{temp_base_name}.mp3\")\n",
        "\n",
        "        if os.path.exists(expected_temp_file):\n",
        "            # Rename the temp file to the final desired path (e.g., audio.mp3)\n",
        "            if os.path.exists(final_output_path):\n",
        "                os.remove(final_output_path) # Remove existing file if present\n",
        "            os.rename(expected_temp_file, final_output_path)\n",
        "            print(f\"‚úÖ Audio downloaded and saved to {final_output_path}.\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"üî¥ Error: Expected audio file {expected_temp_file} not found after download.\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        # Check for specific \"Video unavailable\" errors\n",
        "        if \"Video unavailable\" in str(e):\n",
        "            print(f\"üî¥ Error downloading audio: Video unavailable or restricted.\")\n",
        "        else:\n",
        "            print(f\"üî¥ Error downloading audio with yt-dlp: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "ZNiVpB_JECIC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# AGENT TOOL 3: GEMINI SUMMARIZATION AND JSON STRUCTURING\n",
        "# =================================================================\n",
        "\n",
        "def get_ai_json_schema():\n",
        "    \"\"\"Defines the required structured JSON output for the agent.\"\"\"\n",
        "    return {\n",
        "        \"type\": \"ARRAY\",\n",
        "        \"description\": \"A list of contextually grouped video segments with summaries.\",\n",
        "        \"items\": {\n",
        "            \"type\": \"OBJECT\",\n",
        "            \"properties\": {\n",
        "                \"startTime\": {\"type\": \"STRING\", \"description\": \"The exact start time of the segment in HH:MM:SS format.\"},\n",
        "                \"endTime\": {\"type\": \"STRING\", \"description\": \"The exact end time of the segment in HH:MM:SS format.\"},\n",
        "                \"actualContent\": {\"type\": \"STRING\", \"description\": \"The full transcribed content of the segment.\"},\n",
        "                \"summary\": {\"type\": \"STRING\", \"description\": \"A concise, 10-15 word summary of the main topic and context of this segment, ideal for a video title or chapter marker.\"}\n",
        "            },\n",
        "            \"propertyOrdering\": [\"startTime\", \"endTime\", \"actualContent\", \"summary\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "def summarize_and_structure_transcript(grouped_segments):\n",
        "    \"\"\"\n",
        "    Calls the Gemini API to structure the transcript data and generate summaries,\n",
        "    using exponential backoff for robustness.\n",
        "    \"\"\"\n",
        "    if not grouped_segments:\n",
        "        return []\n",
        "\n",
        "    # Prepare input for the LLM\n",
        "    formatted_input = \"\\n\".join([\n",
        "        f\"Segment {i+1} [Time: {format_time(seg['start'])} to {format_time(seg['end'])}]: {seg['text']}\"\n",
        "        for i, seg in enumerate(grouped_segments)\n",
        "    ])\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are an expert video content analyst. Your task is to review the provided contextually grouped video segments. \"\n",
        "        \"For each segment, you must generate a high-quality, concise summary (max 15 words) suitable for a chapter title. \"\n",
        "        \"Return the output as a single, raw JSON array following the provided schema. Do not include any text outside the JSON array.\"\n",
        "    )\n",
        "\n",
        "    user_query = (\n",
        "        f\"Analyze the following video segments. For each segment, generate a concise summary. \"\n",
        "        f\"The 'startTime', 'endTime', and 'actualContent' fields must be derived directly from the input provided below, \"\n",
        "        f\"formatted as HH:MM:SS. \\n\\nSegments:\\n{formatted_input}\"\n",
        "    )\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": user_query}]}],\n",
        "        \"systemInstruction\": {\"parts\": [{\"text\": system_prompt}]},\n",
        "        \"generationConfig\": {\n",
        "            \"responseMimeType\": \"application/json\",\n",
        "            \"responseSchema\": get_ai_json_schema()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            # We use a standard requests call here\n",
        "            response = requests.post(f\"{API_URL}?key={API_KEY}\", headers=headers, data=json.dumps(payload))\n",
        "            response.raise_for_status()\n",
        "\n",
        "            result = response.json()\n",
        "            json_text = result.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text')\n",
        "\n",
        "            if json_text:\n",
        "                # The LLM's output is the final structured JSON\n",
        "                return json.loads(json_text)\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"API Attempt {attempt + 1} failed: {e}\")\n",
        "            if attempt < 2:\n",
        "                time.sleep(2 ** attempt)\n",
        "            else:\n",
        "                raise Exception(\"üî¥ Failed to get response from Gemini API after multiple retries.\")\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"üî¥ Error: LLM returned malformed JSON. Retrying.\")\n",
        "            if attempt == 2:\n",
        "                raise Exception(\"üî¥ Failed: LLM consistently returned malformed JSON.\")\n",
        "\n",
        "    return []"
      ],
      "metadata": {
        "id": "Lk-3aRMzEG57"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# MAIN EXECUTION BLOCK\n",
        "# =================================================================\n",
        "\n",
        "def main_agent_run():\n",
        "    # 0. Setup Environment\n",
        "    install_dependencies()\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    video_id = get_video_id(YOUTUBE_URL)\n",
        "\n",
        "    if not video_id:\n",
        "        print(f\"üî¥ Error: Could not extract a valid video ID from the URL: {YOUTUBE_URL}\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n--- Starting Video Agent Pipeline for ID: {video_id} ---\")\n",
        "\n",
        "    # 1. ATTEMPT TRANSCRIPTION (Intelligent Fallback)\n",
        "    raw_segments = run_youtube_caption_api(video_id)\n",
        "\n",
        "    # Store initial segment count to detect monolithic output from YT API\n",
        "    initial_segment_count = len(raw_segments) if raw_segments else 0\n",
        "\n",
        "    # Decide if we need to fall back to Whisper\n",
        "    is_monolithic = raw_segments and initial_segment_count == 1\n",
        "\n",
        "    if raw_segments is None or is_monolithic:\n",
        "        if raw_segments is None:\n",
        "            print(\"‚ùå Fallback: YouTube API failed, initiating Whisper workflow.\")\n",
        "        else: # Handle the single-segment issue from auto-captions\n",
        "            print(\"‚ùå Fallback: YT API returned a single, monolithic segment. Initiating chunking/Whisper fallback.\")\n",
        "\n",
        "        # Fallback triggered: Download and run Whisper\n",
        "        if download_audio_with_yt_dlp(YOUTUBE_URL, AUDIO_FILE):\n",
        "            raw_segments = run_whisper_transcription(AUDIO_FILE)\n",
        "\n",
        "    if not raw_segments:\n",
        "        print(\"üî¥ Process stopped: No transcript could be generated by either method.\")\n",
        "        return\n",
        "\n",
        "    # 1.5. Mandatory Time-Based Segmentation (Breaks long blocks from auto-captions/monolithic Whisper)\n",
        "    # This ensures no single segment is too long for the LLM to process contextually.\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # FIX: If the data came from the YT API in small, good chunks (len > 1),\n",
        "    # we don't need to force time-based chunking. If it came from the monolithic\n",
        "    # block (YT or Whisper fallback), we use time-based chunking and skip context grouping.\n",
        "    # --------------------------------------------------------------------------\n",
        "\n",
        "    if initial_segment_count == 1:\n",
        "        # If the input was monolithic (YT API error or fresh Whisper), chunk it by time.\n",
        "        segments_for_llm = split_segments_by_time(raw_segments, max_segment_duration=60.0)\n",
        "        print(f\"‚úÖ Time-Based Chunking Complete: {len(segments_for_llm)} manageable segments created.\")\n",
        "\n",
        "        # We skip the context grouping step because the time-based chunking provides\n",
        "        # the required breaks, and context grouping would merge them back together.\n",
        "        grouped_segments = segments_for_llm\n",
        "        print(f\"‚úÖ Final Segments Prepared (Time-Chunked): {len(grouped_segments)} segments found.\")\n",
        "\n",
        "    else:\n",
        "        # If the input was already fragmented (good YT API output or good Whisper output),\n",
        "        # perform the contextual grouping to merge short pauses.\n",
        "        grouped_segments = group_segments_by_context(raw_segments, max_gap_sec=2.5)\n",
        "        print(f\"‚úÖ Context Grouping Complete: {len(grouped_segments)} logical segments found.\")\n",
        "\n",
        "\n",
        "    # 3. GEMINI AGENT CALL (Summarization and JSON Structuring)\n",
        "    print(\"\\n--- Sending Data to Gemini Agent for Summarization & Structuring ---\")\n",
        "\n",
        "    final_json_data = summarize_and_structure_transcript(grouped_segments)\n",
        "\n",
        "    # 4. FINAL OUTPUT\n",
        "    if final_json_data:\n",
        "        # 4a. Save locally\n",
        "        with open(JSON_OUTPUT_FILE_PATH, 'w', encoding='utf-8') as f:\n",
        "            json.dump(final_json_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(\"\\n---------------------------------------------------------\")\n",
        "        print(\"‚úÖ AGENT SUCCESS: Final JSON Output Generated.\")\n",
        "        print(f\"File saved locally to: {JSON_OUTPUT_FILE_PATH}\")\n",
        "        print(\"\\n--- JSON Preview ---\")\n",
        "        print(json.dumps(final_json_data, ensure_ascii=False, indent=2))\n",
        "        print(\"---------------------------------------------------------\")\n",
        "\n",
        "        # 4b. Upload to Google Drive\n",
        "        upload_to_drive(JSON_OUTPUT_FILE_PATH, JSON_OUTPUT_FILE_NAME)\n",
        "\n",
        "    else:\n",
        "        print(\"üî¥ AGENT FAILED: The LLM did not return the final structured data.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_agent_run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqRNvdx5ELqt",
        "outputId": "bc13eafe-82e7-498f-dd45-081577761209"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Installing Dependencies ---\n",
            "\n",
            "--- Starting Video Agent Pipeline for ID: h3M00JI8Iwo ---\n",
            "‚úÖ Success: Retrieved segments using YouTube Captions API.\n",
            "‚úÖ Context Grouping Complete: 3 logical segments found.\n",
            "\n",
            "--- Sending Data to Gemini Agent for Summarization & Structuring ---\n",
            "\n",
            "---------------------------------------------------------\n",
            "‚úÖ AGENT SUCCESS: Final JSON Output Generated.\n",
            "File saved locally to: /content/transcription_output/video_analysis.json\n",
            "\n",
            "--- JSON Preview ---\n",
            "[\n",
            "  {\n",
            "    \"startTime\": \"00:00:04\",\n",
            "    \"endTime\": \"00:09:21\",\n",
            "    \"actualContent\": \"I am billed as the world's greatest mind reader. But guess what? I can't read minds. What I can do is read people. And people ask a question all the time, which is, ‚ÄúWere you born with this?‚Äù And the answer is no, of course not. Absolutely not. I do not possess any supernatural powers. I am not a psychic. This is a learnable skill that I feel anyone could do, but I've applied for about roughly three decades at reverse-engineering the human mind. If I know how you think, I know what you think. And I want to ask each and every one of you in this room a question. But before we do, take a deep breath in. Take a deep breath in. Exhale out and close your eyes. A little zen mode, OK. I see the people with trust issues, eyes wide open, holding their wallets and phones. Fellow New Yorkers. (Laughter) Here is my question. If you could have dinner, hypothetical, with someone famous, right? Some of you have heard this question already, I didn't invent it, I love asking people, someone famous, dead or alive, past or present, man, woman, whatever. I like to ask people, who would that person be for you? It‚Äôs fascinating, everyone‚Äôs different, and if you've already done it, open your eyes. Some of you have done it, some of you are quick, I see nods, I see people that are decisive. If you haven't, you always need a deadline for creativity. Three, two, one, crunch time, open your eyes. Everyone out there, give me a big clap over your head if you can see your person in your mind's eye, give me a big clap. (Claps) One more big clap. Notice the technique. Elbows locked on this man. Michael Phelps would have been jealous of that streamline. Stand up, please. Introduce yourself. Person: I'm Ian. OP: Ian, question for you. Before you walked in this room and sat down, did you have any idea who I was or what I was going to do? Ian: No, actually, sorry. OP: OK, that's cool, nobody reads the agenda. Totally fine. Give me another clap. I like that, right here, you not only clap, but threw in a little belly dance. I reward extra credit. She's hiding her name tag. She's like, \\\"I'm giving the mentalist nothing.\\\" What is your name? Person: Oh, hi, I'm Nanjera. OP: Nanjera? Stand up please. Did I say that correctly, Nanjera? Wonderful. Give me another clap. One more clap. This man and I just locked eyes. When we made eye contact, he closed his eyes tightly. (Laughter) As if activating a camouflage shield where if he doesn't see me, would I still see him? I still see you, Jeff Johnson, stand up, please. Give all three helpers a round of applause. (Applause) Notice the question and the way it was phrased. Dead or alive? Fifty-fifty, like flipping a coin. All three of you, please. I don't read minds, I'm telling you the truth. I read people. Try not to react. If the person you thought of is alive, then their heart is beating in their chest, the room is getting warm, like the blood flowing through their veins, they're happy, smiling, alive, warming up. Cold? You feel a shiver down your neck. Clear cut, 100 percent, Jeff Johnson, your person is alive and well, am I right? JJ: You're right. OP: The boisterous smile, the effervescence, 100 percent, versus Ian, dead guy vibes written all over this guy. Is it a dead guy? Ian: Yeah. OP: Classic Ian, classic. (Laughter) Ian, focus on this -- Also, the lingering hands in pockets, always indicator of another guy. Guy-to-guy interaction. Is it a guy? Ian: Yes. OP: Of course it is. If it's a female, hands in front of the crotch, I don't know why. (Laughter) A few of you are going to get home, and you see your husband like this, and you're like, \\\"Who is she?\\\" (Laughter) Ian, think of his first name, think of his last name. First name, last name. Come back to me. The first name, try to count the letters. This doesn't make sense. Listen to me. This doesn't make sense. He was confused when I asked about the last name. He tried to shield it, but I saw it. Why was he confused? Then I asked the first name, count the letters. It was too long. Sit down if it's Alexander the Great. Ian: What? (Laughter and scattered applause) You don't have to hold the applause inside, TED. You can let it out. (Applause and cheers) You've seen a taste of what I do. And this is used primarily for the purpose of entertainment. But what I'd like to show you now is how you could apply the principles of my craft, which is known as mentalism, to your everyday lives. At home, at work, in your relationships. How would guessing a celebrity out of a stranger's mind help you? Well, frankly, it really can't. (Laughter) But I have a situation, a scenario where knowing someone's name would be incredibly valuable. Let me paint a portrait. You're at a party, you're at a work event, you've walked up to somebody new, you've looked them in the eye, shaken their hands, introduced yourselves to each other, and right at that moment, you realize that you have completely forgotten the person's name who just told it to you one second ago. Raise your hand if you can share this experience. You raise your hand, too, you liar. She's like, \\\"Not me, not me.\\\" I want to give you a superpower, from this day forward, where you will never, ever forget the name of someone you just met. Ever. And how am I going to do this? Using, can you guess? Shampoo. Didn't see that one coming, did you? The back of every bottle of shampoo is a masterclass in brilliant marketing. Three words in the instructions: lather, rinse, repeat. Lather, makes your hair smell good; rinse makes it clean; repeat -- we've got to sell some product. I want to repurpose that catchphrase that each and every one of you will remember and take with you for the rest of your days. Listen. Repeat. Reply. Say it with me, listen, repeat, reply. Listen sounds like the most obvious thing in the world. Of course, listen to the name. But that's where most of you falter. At the moment the person tells you the name, you are actually not listening. You are thinking about what you're going to say back. Do I know this person or what am I going to say next? Or a million other thoughts that pop into your head. It's not a memory issue. You didn't forget the name, you never knew it. Imagine I give you a twig and I tell you, write my name in the sand at the beach. And you try with the twig. First wave washes it away. But if I give you a thick branch, you carve it in. You repeat those letters. It's going to take a whole lot of waves until there's no trace. So here's what each of you is going to do. When you meet the next person, make your mind a blank for all of two seconds. When they say their name, truly listen. And now repeat. Everybody say it with me, repeat. We are going to say their name twice back to them, right? Ashley, is that right? Great to meet you Ashley. This serves two purposes. One, it kind of repetitively ingrains it in our memory. Second, you know how to pronounce it correctly. Nanjera? Am I saying that right, Nanjera? See, there was a reason I did that. Third, third: reply. Listen, repeat? Reply. Reply is where we fully cement the name in. Here is how we do that, I'll give you three examples. First one, a compliment. Everyone loves a good compliment, right? \\\"Ashley, those earrings, I love those earrings, Ashley.\\\" Now she's Ashley with the earrings. A visual indicator. Next up, spelling. \\\"Do you spell that Ashley with an EIGH or with an EY? EY, of course, that's the right spelling, Ashley, I knew.\\\" Or, third, a personal connection. \\\"Ashley, that's so funny. My cousin‚Äôs married to Ashley.‚Äù Is my cousin really married to Ashley? Absolutely not, I'm a total liar. (Laughter) But none of you will forget the name Ashley. And neither will I. And neither will we forget Nanjera, after I said her name multiple times. Nanjera, I think -- again, rouging of the cheeks, little sweat. Your person is alive also. Nanjera: Correct. OP: Now, what I do in my profession is I take things that appear to have limitless possibilities and break them down. People that are famous, there are several, kind of, categories. I'm going to say them quickly, you don't have to nod, but we had a conqueror, let's call it a historical figure. We have singers, movie stars, athletes, comedians, politicians, already registered a hit. Too funny you did this. Alright. Shouldn't have smiled, tighten it up, Nanjera. (Laughter) Three days ago, I did a show in Dallas, Texas. Dallas, Texas. And the last person standing thought of a singer. It's always singers where this happens. I got his singer correct and he didn't sit down. And do you know what he said to me? I go, you know, why isn't he sitting down? He goes, \\\"Oh yeah? What song am I thinking of?\\\" (Laughter) I'm like, buddy, this is not Netflix. (Laughter) Always singers.\",\n",
            "    \"summary\": \"How to read people: The mentalist guesses a historical figure and teaches the LRR name memory method.\"\n",
            "  },\n",
            "  {\n",
            "    \"startTime\": \"00:09:24\",\n",
            "    \"endTime\": \"00:14:12\",\n",
            "    \"actualContent\": \"Tell us all, who, in your mind, is sitting across from you, having dinner? You're on the edge of your seat. What's their name? Nanjera: Bob Dylan. OP: I couldn't hear you. Nanjera: Bob Dylan. OP: Can I ask you a question? I always like this. Everyone, listen to me. I call this the grass is greener approach. I can always see it when someone's eyes shift. She goes, \\\"Mm, I like Bob Dylan, but I actually had someone else in my mind.\\\" You thought of someone else. It's in the back of your head, but you just shifted. Can you tell me -- Folks -- (Laughter) This is not my first rodeo. (Laughter) Who was the first person you thought of? (Laughter) Tell us. Nanjera: Trevor Noah. OP: Ooh. It's not a singer. Ooh. Don't ever doubt me again, TED, don't you ever doubt me again. (Applause and cheers) Jeff. Do you know what the most common question I get at the end of a show is? Other than, of course, \\\"How do you do it,\\\" is, \\\"Aren't you afraid of getting it wrong?\\\" That's what I get asked, because people understand that there is risk involved in what I do. This is not sleight-of-hand card tricks that work every time. And so the answer is, yeah, of course, of course I am. You know what? Because what's fascinating to me is people can feel the element of risk. And I think the number one factor in success, both on stage and in every facet of my life, is an unshakable belief that it will work. It's self-fulfilling. And in this case, it's not a belief that I'm going to guess your person. Been there, done that. I don't plateau, Jeff, I peak. So I am not going to read your mind. I want to empower someone in this room to do it instead. This Frisbee -- (Laughter) Was purchased on Amazon Prime, free two-day shipping, 87 cents. It has a 1.5-star review. (Laughter) This is literally the worst Frisbee in the world. (Laughter) Please, sir, grab that. Tell me your first name. Person: Dylan. OP: That's correct. (Laughter) Stand up, Dylan. The Frisbee is made out of mesh, everybody. Which means if it hits you in the face, it will not hurt. But it will hurt your pride if this goes viral on TED Talks later. Everyone hands up in defensive posture. Dylan, close your eyes so you can't even try to aim this piece of junk. Throw it. Someone catch. Give it a throw, please go. Oh, my God, is that far? Mike runner, run, run, run. Stand up, stand up! (Applause) Please stand up. What is your name? Did you say Brett? Person: Yeah, Brett. OP: Have you ever met or spoken a word to Jeff Johnson before? Brett: I actually haven't. OP: OK, maybe more networking at the next breakout session. (Laughter) Please, Brett, come on up. Give him a huge round of applause, make your way up, Brett. Two Ts, I like the way you roll, Brett. Not going to forget that name, are you now? Pleasure to meet you, sir. Brett, I‚Äôm bringing you front and center. Let's adjust this to the height my wife wishes I had. (Laughter) She is watching this now live and saying \\\"That's true.\\\" Brett, here's what you're going to do. I'm going to ask you in a moment -- Have you ever spoken to me a word in your life? Brett: I haven't. OP: He seemed happy about that, that was weird. I'm going to ask him in a moment to close his eyes. You need to believe it in order to achieve it. When you close your eyes, I am going to ask Jeff to uncap a marker. And you're going to write the person. Very important, Jeff, you thought of one person, is that correct? Jeff: That's right. OP: Because, you know, Nanjera is like, having dinner. She's like, load it up, let's get a six top. Brett, close your eyes. His eyes are closed. Brett, keep your eyes closed. Jeff, open up your marker, please. As big as you can, so we can't see it, write down the first and last name of the person you are imagining sitting down to dinner with. Brett, open your eyes. You have to believe it in order to achieve it. Look, not into his eyes, look into his soul. And when I snap my fingers, tell him, who is he having dinner with? Brett: Barack Obama. OP: Turn it around! Show everybody, show everybody. (Cheers and applause) Thank you, Brett. (Cheers and applause)\",\n",
            "    \"summary\": \"The mentalist correctly guesses Trevor Noah, then empowers an audience member to reveal Barack Obama.\"\n",
            "  },\n",
            "  {\n",
            "    \"startTime\": \"00:14:18\",\n",
            "    \"endTime\": \"00:14:21\",\n",
            "    \"actualContent\": \"I am Oz Pearlman, thank you very much, everybody.\",\n",
            "    \"summary\": \"Closing remarks and introduction of Oz Pearlman.\"\n",
            "  }\n",
            "]\n",
            "---------------------------------------------------------\n",
            "\n",
            "--- Starting Google Drive Upload ---\n",
            "Mounted at /content/gdrive\n",
            "\n",
            "-------------------------------------------\n",
            "‚úÖ Upload Successful!\n",
            "File saved to Google Drive at: MyDrive/AI_Transcripts/video_analysis.json\n",
            "-------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0HLY9zIGEPIq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}